{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from IPython.display import clear_output\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch import nn\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-MET': 0,\n",
       " 'B-ECO': 1,\n",
       " 'B-BIN': 2,\n",
       " 'B-CMP': 3,\n",
       " 'B-QUA': 4,\n",
       " 'B-ACT': 5,\n",
       " 'B-INST': 6,\n",
       " 'B-SOC': 7,\n",
       " 'I-MET': 8,\n",
       " 'I-ECO': 9,\n",
       " 'I-BIN': 10,\n",
       " 'I-CMP': 11,\n",
       " 'I-QUA': 12,\n",
       " 'I-ACT': 13,\n",
       " 'I-INST': 14,\n",
       " 'I-SOC': 15,\n",
       " 'O': 16}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_idx2labels = ['B-MET', 'B-ECO', 'B-BIN', 'B-CMP', 'B-QUA', 'B-ACT', 'B-INST', 'B-SOC',\n",
    "                  'I-MET', 'I-ECO', 'I-BIN', 'I-CMP', 'I-QUA', 'I-ACT', 'I-INST', 'I-SOC', 'O']\n",
    "ent_labels2idx = dict(zip(ent_idx2labels, range(len(ent_idx2labels))))\n",
    "ent_labels2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('MalakhovIlya/RuREBus', split='train')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê \\n–†–ï–°–ü–£–ë–õ–ò–ö–ò –ö–û–ú–ò \" ...</td>\n",
       "      <td>[T1\\tSOC 259 280\\t–ü—Ä–∞–≤–æ–≤–∞—è –∑–∞—â–∏—â–µ–Ω–Ω–æ—Å—Ç—å, T4\\tI...</td>\n",
       "      <td>[R1\\tTSK Arg1:T71 Arg2:T72, R2\\tGOL Arg1:T74 A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê \\n–†–ï–°–ü–£–ë–õ–ò–ö–ò –ö–û–ú–ò \" ...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [T1\\tSOC 259 280\\t–ü—Ä–∞–≤–æ–≤–∞—è –∑–∞—â–∏—â–µ–Ω–Ω–æ—Å—Ç—å, T4\\tI...   \n",
       "\n",
       "                                           relations  \n",
       "0  [R1\\tTSK Arg1:T71 Arg2:T72, R2\\tGOL Arg1:T74 A...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_dataset)\n",
    "train_df.drop(columns='id', inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation_list(df: pd.DataFrame, idx: int):\n",
    "    relation_list = []\n",
    "    for relation in df['relations'].iloc[idx]:\n",
    "        vals = relation.replace(':', ' ').split()\n",
    "        rel_id, cls_id, _, subj_id, _, obj_id = vals\n",
    "        relation_list.append([\n",
    "            rel_id, cls_id, subj_id, obj_id\n",
    "        ])\n",
    "    return relation_list\n",
    "\n",
    "def get_sorted_entity_list(df: pd.DataFrame, idx: int):\n",
    "    entity_list = []\n",
    "    for entity in df['entities'].iloc[idx]:\n",
    "        vals = entity.split()\n",
    "        ent_id, cls_id, b_idx, e_idx = vals[0:4]\n",
    "        b_idx, e_idx = int(b_idx), int(e_idx)\n",
    "        words = vals[4:]\n",
    "        \n",
    "        entity_list.append([\n",
    "            b_idx, e_idx, cls_id, words, ent_id\n",
    "        ])\n",
    "    \n",
    "    entity_list = sorted(entity_list)\n",
    "    return entity_list\n",
    "\n",
    "def replace_entities_and_relations(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    for i in range(len(df)):\n",
    "        df.at[i, 'entities'] = get_sorted_entity_list(df, i)\n",
    "        df.at[i, 'relations'] = get_relation_list(df, i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity ex.: [45, 52, 'SOC', ['–Æ–°–¢–ò–¶–ò–Ø'], 'T82']\n",
      "Relation ex.: ['R1', 'TSK', 'T71', 'T72']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê \\n–†–ï–°–ü–£–ë–õ–ò–ö–ò –ö–û–ú–ò \" ...</td>\n",
       "      <td>[[45, 52, SOC, [–Æ–°–¢–ò–¶–ò–Ø], T82], [55, 66, BIN, ...</td>\n",
       "      <td>[[R1, TSK, T71, T72], [R2, GOL, T74, T73], [R3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê \\n–†–ï–°–ü–£–ë–õ–ò–ö–ò –ö–û–ú–ò \" ...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [[45, 52, SOC, [–Æ–°–¢–ò–¶–ò–Ø], T82], [55, 66, BIN, ...   \n",
       "\n",
       "                                           relations  \n",
       "0  [[R1, TSK, T71, T72], [R2, GOL, T74, T73], [R3...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = replace_entities_and_relations(train_df)\n",
    "print('Entity ex.:', train_df.at[0, 'entities'][0])\n",
    "print('Relation ex.:', train_df.at[0, 'relations'][0])\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_labeled_text(df: pd.DataFrame, idx: int):\n",
    "    margin = 0\n",
    "    text = df.at[idx, 'text']\n",
    "    for entity in df.at[idx, 'entities']:\n",
    "        b_idx, e_idx, cls_id, words, _ = entity\n",
    "        b_idx, e_idx = b_idx + margin, e_idx + margin\n",
    "        \n",
    "        text_len_in_beginning = len(text)\n",
    "        text = text[:b_idx] + f'__B-{cls_id}__' + text[b_idx:e_idx] + f'__E-{cls_id}__' + text[e_idx:]\n",
    "        margin += len(text) - text_len_in_beginning\n",
    "        \n",
    "    return text\n",
    "\n",
    "def get_splited_text_and_ner_tags(df: pd.DataFrame, idx: int):\n",
    "    ner_tags = []\n",
    "    words = get_entity_labeled_text(df, idx).split()\n",
    "    \n",
    "    e_padded_label = None\n",
    "    for word_idx, word in enumerate(words):\n",
    "        if e_padded_label is not None:\n",
    "            i_label = e_padded_label.replace('_', '').replace('E-', 'I-')\n",
    "            ner_tags.append(ent_labels2idx[i_label])\n",
    "            if e_padded_label in word:\n",
    "                words[word_idx] = words[word_idx].replace(e_padded_label, '')\n",
    "                e_padded_label = None\n",
    "            continue\n",
    "        \n",
    "        b_padded_label = None\n",
    "        for b_label in ent_idx2labels[:8]:\n",
    "            b_padded_label = f'__{b_label}__'\n",
    "            if b_padded_label in word:\n",
    "                words[word_idx] = words[word_idx].replace(b_padded_label, '')\n",
    "                ner_tags.append(ent_labels2idx[b_label])\n",
    "                e_padded_label = b_padded_label.replace('B-', 'E-')\n",
    "\n",
    "        if e_padded_label is not None and e_padded_label in word:\n",
    "            words[word_idx] = words[word_idx].replace(e_padded_label, '')\n",
    "            e_padded_label = None\n",
    "            continue\n",
    "        elif e_padded_label is None:\n",
    "            ner_tags.append(ent_labels2idx['O'])\n",
    "    \n",
    "    return words, ner_tags\n",
    "\n",
    "def replace_text_and_add_entity_tags(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df['ner_tags'] = None\n",
    "    for i in range(len(df)):\n",
    "        words, ner_tags = get_splited_text_and_ner_tags(df, i)\n",
    "        df.at[i, 'text'] = words\n",
    "        df.at[i, 'ner_tags'] = ner_tags\n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER tags ex.: [16, 16, 4, 16, 16, 6, 16, 16, 16, 16, 16, 6, 14, 14, 16, 16, 1, 9, 16, 4, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, 12, 16, 1, 9, 16, 2, 5, 13, 16, 16, 16, 16, 16, 16, 16, 16, 5, 13, 13, 13, 13, 2, 16, 16, 16, 6, 16, 6, 14, 14, 14, 16]\n",
      "Text ex.: ['–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '–∑–∞', '–¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º—ã—Ö', '–≤', '–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '—Å–≤–µ–¥–µ–Ω–∏–π', '–∏', '–æ—Ç—á–µ—Ç–æ–≤', '–≤–æ–∑–ª–∞–≥–∞–µ—Ç—Å—è', '–Ω–∞', '–æ—Ä–≥–∞–Ω—ã', '–º–µ—Å—Ç–Ω–æ–≥–æ', '—Å–∞–º–æ—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è', '.', '4.', '–°—Ä–µ–¥—Å—Ç–≤–∞', '—Å—É–±–≤–µ–Ω—Ü–∏–π', '—è–≤–ª—è—é—Ç—Å—è', '—Ü–µ–ª–µ–≤—ã–º–∏', '–∏', '–Ω–µ', '–º–æ–≥—É—Ç', '–±—ã—Ç—å', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã', '–ø–æ', '–∏–Ω–æ–º—É', '–Ω–∞–∑–Ω–∞—á–µ–Ω–∏—é', '.', '–ù–µ', '—Ü–µ–ª–µ–≤–æ–µ', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ', '—Å—Ä–µ–¥—Å—Ç–≤', '—Å—É–±–≤–µ–Ω—Ü–∏–π', '–≤–ª–µ—á–µ—Ç', '–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ', '–º–µ—Ä', '–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏', '–≤', '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏', '—Å', '–∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º', '–†–æ—Å—Å–∏–π—Å–∫–æ–π', '–§–µ–¥–µ—Ä–∞—Ü–∏–∏', '.', '5.', '–ö–æ–Ω—Ç—Ä–æ–ª—å', '–∑–∞', '—Ü–µ–ª–µ–≤—ã–º', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º', '—Å—É–±–≤–µ–Ω—Ü–∏–π', '–æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è', '–≤', '—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–º', '–ø–æ—Ä—è–¥–∫–µ', '–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ–º', '–∏', '–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ–º', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤', '–†–µ—Å–ø—É–±–ª–∏–∫–∏', '–ö–æ–º–∏', '.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>relations</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[–ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–ê–Ø, –ü–†–û–ì–†–ê–ú–ú–ê, –†–ï–°–ü–£–ë–õ–ò–ö–ò, –ö–û–ú–ò,...</td>\n",
       "      <td>[[45, 52, SOC, [–Æ–°–¢–ò–¶–ò–Ø], T82], [55, 66, BIN, ...</td>\n",
       "      <td>[[R1, TSK, T71, T72], [R2, GOL, T74, T73], [R3...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 7, 16, 2, 7, 16, 16, 16, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [–ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–ê–Ø, –ü–†–û–ì–†–ê–ú–ú–ê, –†–ï–°–ü–£–ë–õ–ò–ö–ò, –ö–û–ú–ò,...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [[45, 52, SOC, [–Æ–°–¢–ò–¶–ò–Ø], T82], [55, 66, BIN, ...   \n",
       "\n",
       "                                           relations  \\\n",
       "0  [[R1, TSK, T71, T72], [R2, GOL, T74, T73], [R3...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [16, 16, 16, 16, 16, 7, 16, 2, 7, 16, 16, 16, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = replace_text_and_add_entity_tags(train_df)\n",
    "print('NER tags ex.:', train_df.at[2, 'ner_tags'])\n",
    "print('Text ex.:', train_df.at[2, 'text'])\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590d864aa596454da073ecce0b265929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(example, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(example['text'], truncation=True, is_split_into_words=True, padding='max_length', max_length=512) \n",
    "    labels = [] \n",
    "    for i, label in enumerate(example['ner_tags']): \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) \n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token. \n",
    "        previous_word_idx = None \n",
    "        label_ids = []\n",
    "        # Special tokens like `<s>` and `<\\s>` are originally mapped to None \n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids: \n",
    "            if word_idx is None: \n",
    "                # set ‚Äì100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token                 \n",
    "                label_ids.append(label[word_idx]) \n",
    "            else: \n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100) \n",
    "                # mask the subword representations after the first subword\n",
    "                 \n",
    "            previous_word_idx = word_idx \n",
    "        labels.append(label_ids) \n",
    "    tokenized_inputs[\"labels\"] = labels \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def split_texts(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    rows_to_drop = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        text = df.at[i, 'text']\n",
    "        ner_tags = df.at[i, 'ner_tags']\n",
    "        \n",
    "        if len(text) > 30:\n",
    "            rows_to_drop.append(i)\n",
    "            \n",
    "            num_of_parts = len(text) // 30\n",
    "            part_size = len(text) // num_of_parts\n",
    "            \n",
    "            past_margin = 0\n",
    "            for i in range(num_of_parts - 1):\n",
    "                part_end = part_size*i + part_size - 1\n",
    "                cur_margin = 0\n",
    "                while ner_tags[part_end - cur_margin] != ent_labels2idx['O']:\n",
    "                    cur_margin += 1\n",
    "                text_part = text[part_size*i - past_margin:part_size*i + part_size - cur_margin]\n",
    "                ner_tags_part = ner_tags[part_size*i - past_margin:part_size*i + part_size - cur_margin]\n",
    "                if len(text_part) > 0:\n",
    "                    df = df.append({'text': text_part, 'ner_tags': ner_tags_part}, ignore_index=True)\n",
    "                past_margin = cur_margin\n",
    "            \n",
    "            text_part = text[part_size*(num_of_parts - 1) - past_margin:]\n",
    "            ner_tags_part = ner_tags[part_size*(num_of_parts - 1) - past_margin:]\n",
    "            if len(text_part) > 0:\n",
    "                df = df.append({'text': text_part, 'ner_tags': ner_tags_part}, ignore_index=True)\n",
    "        \n",
    "    df.drop(rows_to_drop, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # return Dataset.from_pandas(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "train_dataset_formated = Dataset.from_pandas(split_texts(train_df[['text', 'ner_tags']]))\n",
    "tokenized_train_dataset = train_dataset_formated.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9407650615794819"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(pd.Series(tokenized_train_dataset['input_ids']).explode(), pd.Series(tokenized_train_dataset['labels']).explode().astype(str))\n",
    "dummy_clf.score(pd.Series(tokenized_train_dataset['input_ids']).explode(), pd.Series(tokenized_train_dataset['labels']).explode().astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_values=pd.Series(tokenized_train_dataset['labels']).explode()\n",
    "exploded_values=pd.DataFrame(exploded_values,columns=['B'])\n",
    "\n",
    "most_frequent_elem_by_doc=pd.Series(tokenized_train_dataset['labels']).apply(lambda x:  max(set(x), key=x.count))\n",
    "most_frequent_elem_by_doc=pd.DataFrame(most_frequent_elem_by_doc,columns=list('A'))\n",
    "\n",
    "df_most_freq_token=exploded_values.merge(most_frequent_elem_by_doc, how='right', left_index=True, right_index=True)\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(pd.Series(tokenized_train_dataset['input_ids']).explode(), df_most_freq_token['A'])\n",
    "dummy_clf.score(pd.Series(tokenized_train_dataset['input_ids']).explode(), df_most_freq_token['A'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaBSE NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, hf_dataset, num_classes=10):\n",
    "        self.dataset = hf_dataset\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        input_ids = torch.Tensor(self.dataset['input_ids'][idx]).type(torch.LongTensor)\n",
    "        attention_mask = torch.Tensor(self.dataset['attention_mask'][idx])\n",
    "        labels = torch.Tensor(self.dataset['labels'][idx])\n",
    "        \n",
    "        model_input = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "        sample = [model_input, labels]\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerModule(pl.LightningDataModule):\n",
    "    def __init__(self, hf_dataset, batch_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.dataset = hf_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        ner_full = NerDataset(hf_dataset=self.dataset, num_classes=self.num_classes)\n",
    "        train_size = int(0.8*len(ner_full))\n",
    "        test_size = int(0.15*len(ner_full))\n",
    "        val_size = len(ner_full) - train_size - test_size\n",
    "        self.ner_train, self.ner_test, self.ner_val = torch.utils.data.random_split(ner_full, [train_size, test_size, val_size])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.ner_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.ner_test, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.ner_val, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERPredictionLogger(pl.callbacks.Callback):\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.val_inputs, self.val_labels = val_samples\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Bring the tensors to CPU\n",
    "        val_inputs = {\n",
    "            'input_ids': self.val_inputs['input_ids'].to(device=pl_module.device),\n",
    "            'attention_mask': self.val_inputs['attention_mask'].to(device=pl_module.device),\n",
    "        }\n",
    "        val_labels = self.val_labels.to(device=pl_module.device)\n",
    "        # Get model prediction\n",
    "        logits = pl_module(val_inputs)['logits']\n",
    "        gr_truths = []\n",
    "        preds = []\n",
    "        texts = []\n",
    "        for i in range(logits.shape[0]):\n",
    "            clean_inst = val_labels[i] != -100\n",
    "            preds.append(logits[i][clean_inst].argmax(dim=1))\n",
    "            gr_truths.append(val_labels[i][clean_inst].type(torch.LongTensor))\n",
    "            # print(val_labels[i])\n",
    "            # print(val_inputs['input_ids'][i])\n",
    "            # print(val_inputs['input_ids'][i][clean_inst])\n",
    "            # print(tokenizer.decode(val_inputs['input_ids'][i][clean_inst]))\n",
    "            texts.append(tokenizer.decode(val_inputs['input_ids'][i], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "        # Log the results as wandb Table\n",
    "        \n",
    "        table = wandb.Table(columns=['Text', 'True Entities', 'Predicted Entities'])\n",
    "        for i in range(logits.shape[0]):\n",
    "            text = ''.join(texts[i])\n",
    "            gr_truth = ', '.join(list(map(str, gr_truths[i].tolist())))\n",
    "            pred = ', '.join(list(map(str, preds[i].tolist())))\n",
    "            table.add_data(text, gr_truth, pred)\n",
    "        \n",
    "        trainer.logger.experiment.log({\"examples\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerNet(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, learning_rate=2e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # log hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = AutoModelForTokenClassification.from_pretrained('surdan/LaBSE_ner_nerel')\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            # nn.Linear(768, 2048),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(2048, 2048),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(768, num_classes),\n",
    "        )\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # for child_idx, child in enumerate(self.model.children()):\n",
    "        #     if child_idx == 0:\n",
    "        #         pass\n",
    "        #         for g_child_idx, g_child in enumerate(child.children()):\n",
    "        #             if g_child_idx == 1:\n",
    "        #                 for g_g_child_idx, g_g_child in enumerate(next(g_child.children())):\n",
    "        #                     if g_g_child_idx >= 11:\n",
    "        #                         for param in g_g_child.parameters():\n",
    "        #                             param.requires_grad = True\n",
    "        #     else:\n",
    "        #         for param in child.parameters():\n",
    "        #             param.requires_grad = True\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.f1 = MulticlassF1Score(num_classes=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(**x)\n",
    "        return x\n",
    "    \n",
    "    def count_loss_and_metrics(self, y_hat, y):\n",
    "        f1_score = 0\n",
    "        loss = 0\n",
    "        y_hat = y_hat['logits']\n",
    "        for i in range(y.shape[0]):\n",
    "            clean_inst = y[i] != -100\n",
    "            y_hat_cleaned = y_hat[i][clean_inst].cuda()\n",
    "            y_cleaned = y[i][clean_inst].type(torch.LongTensor).cuda()\n",
    "            \n",
    "            f1_score += self.f1(y_hat_cleaned.argmax(dim=1), y_cleaned)\n",
    "            loss += self.loss(y_hat_cleaned.type(torch.FloatTensor), \n",
    "                              F.one_hot(y_cleaned, num_classes=self.num_classes).type(torch.FloatTensor))\n",
    "\n",
    "        return loss, loss / y.shape[0], f1_score / y.shape[0]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss, loss_meaned, f1_score = self.count_loss_and_metrics(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss_meaned, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_F1', f1_score, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss, loss_meaned, f1_score = self.count_loss_and_metrics(y_hat, y)\n",
    "\n",
    "        self.log('val_loss', loss_meaned, prog_bar=True)\n",
    "        self.log('val_F1', f1_score, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss, loss_meaned, f1_score = self.count_loss_and_metrics(y_hat, y)\n",
    "\n",
    "        self.log('test_loss', loss_meaned, prog_bar=True)\n",
    "        self.log('test_F1', f1_score, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = NerModule(hf_dataset=tokenized_train_dataset,\n",
    "               batch_size=2,\n",
    "               num_classes=len(ent_idx2labels))\n",
    "# To access the x_dataloader we need to call prepare_data and setup.\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "# Samples required by the custom ImagePredictionLogger callback to log image predictions.\n",
    "val_samples = next(iter(dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleon-1402\u001b[0m (\u001b[33mcorgi-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20230122_025320-22wbvvh8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/corgi-team/rurebus-ner/runs/22wbvvh8\" target=\"_blank\">jumping-jazz-59</a></strong> to <a href=\"https://wandb.ai/corgi-team/rurebus-ner\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PnthrLeo\\.conda\\envs\\image-processing\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:52: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\PnthrLeo\\.conda\\envs\\image-processing\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:731: LightningDeprecationWarning: `trainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v2.0. Specify the fit checkpoint path with `trainer.fit(ckpt_path=)` instead.\n",
      "  ckpt_path = ckpt_path or self.resume_from_checkpoint\n",
      "Restoring states from the checkpoint path at rurebus-ner/1uxtfhyy/checkpoints/epoch=1-step=1970.ckpt\n",
      "c:\\Users\\PnthrLeo\\.conda\\envs\\image-processing\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:345: UserWarning: The dirpath has changed from '.\\\\rurebus-ner\\\\1uxtfhyy\\\\checkpoints' to '.\\\\rurebus-ner\\\\22wbvvh8\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | BertForTokenClassification | 127 M \n",
      "1 | loss  | BCEWithLogitsLoss          | 0     \n",
      "2 | f1    | MulticlassF1Score          | 0     \n",
      "-----------------------------------------------------\n",
      "127 M     Trainable params\n",
      "0         Non-trainable params\n",
      "127 M     Total params\n",
      "511.070   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at rurebus-ner/1uxtfhyy/checkpoints/epoch=1-step=1970.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03b959abcfd4f538a305c656f9e133b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PnthrLeo\\.conda\\envs\\image-processing\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\PnthrLeo\\.conda\\envs\\image-processing\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c837621a5a45039f32e040d3c6eeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 985it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed03c1a645174dcea597006dd8a03a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PnthrLeo\\.conda\\envs\\image-processing\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "c:\\Users\\PnthrLeo\\.conda\\envs\\image-processing\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1386: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at .\\rurebus-ner\\22wbvvh8\\checkpoints\\epoch=2-step=5908.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at .\\rurebus-ner\\22wbvvh8\\checkpoints\\epoch=2-step=5908.ckpt\n",
      "c:\\Users\\PnthrLeo\\.conda\\envs\\image-processing\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ffea3b19384da28e01816b651d08de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NerNet(num_classes=dm.num_classes)\n",
    "\n",
    "# Initialize wandb logger\n",
    "wandb_logger = WandbLogger(project='rurebus-ner', job_type='train')\n",
    "\n",
    "# Initialize Callbacks\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"val_loss\")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(max_epochs=5,\n",
    "                     accelerator='gpu',\n",
    "                     devices=1,\n",
    "                     logger=wandb_logger,\n",
    "                     resume_from_checkpoint='rurebus-ner/1uxtfhyy/checkpoints/epoch=1-step=1970.ckpt',\n",
    "                     callbacks=[NERPredictionLogger(val_samples),\n",
    "                                checkpoint_callback],\n",
    "                     log_every_n_steps=1,\n",
    "                     )\n",
    "\n",
    "# Train the model ‚ö°üöÖ‚ö°\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "# Evaluate the model on the held-out test set ‚ö°‚ö°\n",
    "trainer.test(dataloaders=dm.test_dataloader())\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct  7 2022, 20:14:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f2d1712ad61a1680ef1e32d8f888e03ab49bcfa8a6ea21125ccac88d7602bd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
